{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "## Class: MSDS 7331 Data Mining\n",
    "## Dataset: Kaggle.com\n",
    "<p>COLLABORATORS: Rebecca Holsapple, Steven Cocke, Iram Bakhtiar</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>1. Business Understanding</font>\n",
    "\n",
    "In today's world, air travel is not just becoming more common for business and leisure trips, but is an integral part of the economy. Air travel has been consistently increasing the past few decades. \n",
    "\n",
    "We believe that a thorough analysis and understanding of delays and cancellations of flights, can provide useful business recommendations to the airline industry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mpl_toolkits.basemap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ec1baed34393>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConnectionPatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mpl_toolkits.basemap'"
     ]
    }
   ],
   "source": [
    "import datetime, warnings, scipy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import OrderedDict\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the data\n",
    "\n",
    "Flights = pd.read_csv(\"C:/Users/DataScience/Documents/SMU/Data_Mining2/Lab1/Flight_Data/flights.csv\", low_memory=False)\n",
    "Airlines = pd.read_csv(\"C:/Users/DataScience/Documents/SMU/Data_Mining2/Lab1/Flight_Data/airlines.csv\", low_memory=False)\n",
    "Airports = pd.read_csv(\"C:/Users/DataScience/Documents/SMU/Data_Mining2/Lab1/Flight_Data/airports.csv\", low_memory=False)\n",
    "Regions =  pd.read_csv(\"C:/Users/DataScience/Documents/SMU/Data_Mining2/Lab1/Flight_Data/state_region.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_flights = Flights['ORIGIN_AIRPORT'].value_counts()\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "colors = ['white', 'blue', 'lightblue', 'yellow', 'red', 'black']\n",
    "flight_limits = [1, 1000, 10000, 50000, 150000, 1000000]\n",
    "\n",
    "labels = []\n",
    "for i in range(len(flight_limits) - 1):\n",
    "    labels.append(\"{} to {}\".format(flight_limits[i], flight_limits[i+1]))\n",
    "\n",
    "map = Basemap(resolution = 'i',llcrnrlon = -180, urcrnrlon = -50, \n",
    "              llcrnrlat = 10, urcrnrlat = 75, lat_0 = 0, lon_0 = 0,)\n",
    "map.shadedrelief()\n",
    "map.drawcoastlines()\n",
    "map.drawcountries(linewidth = 5)\n",
    "map.drawstates(color='0.75')\n",
    "\n",
    "for index, (code, y, x) in Airports[['IATA_CODE', 'LATITUDE', 'LONGITUDE']].iterrows():\n",
    "    x, y = map(x, y)\n",
    "    isize = [i for i, val in enumerate(flight_limits) if val < count_flights[code]]\n",
    "    ind = isize[-1]\n",
    "    map.plot(x, y, marker='s', markersize = ind + 5, markeredgewidth = 1, \n",
    "             color = colors[ind], markeredgecolor='k', label = labels[ind])\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = OrderedDict(zip(labels, handles))\n",
    "key_order = ('1 to 1000', '1000 to 10000', '10000 to 50000', '50000 to 150000', '150000 to 1000000')\n",
    "legend = OrderedDict()\n",
    "for key in key_order:\n",
    "    legend[key] = by_label[key]\n",
    "plt.legend(legend.values(), legend.keys(), loc = 0, prop= {'size':11},\n",
    "           title='Airport: number of flights per year', frameon = True, framealpha = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>2. Data Meaning Type</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the project our data comes from three datasets\n",
    "1. The first has data on airlines. The dataset has 14 rows and 2 columns\n",
    "2. The second has data on airports in the USA. It has 322 rows and 7 columns\n",
    "3. And the last one has information on flights and it has 5.8M rows and 31 columns\n",
    "\n",
    "All data is for 2005 and was obtained form Kaggle. The link to the dataset is [Kaggle.com/US-flight-delays](https://www.kaggle.com/usdot/flight-delays)\n",
    "\n",
    "To describe the meaning and type of data, we'll be making use of the data dictionary below that describes all the attributes that are in the three data sets being used for the lab. We'll also be using code that will show the number of rows and columns of each data set and if the values of the attributes are floats/integers/objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dictionary\n",
    "\n",
    "Below is the description of the attributes for each of the data sets used. For the purposes of this lab, we plan on taking an overall look at the data sets and provide insights at a higher level. The team has decided to continue using the same data sets to glean more meaningful and deeper insights, making use of modeling and machine learning for future labs.\n",
    "\n",
    "### Flights  \n",
    "\n",
    "**YEAR** : Year of the Flight Trip  \n",
    "**MONTH** : Month of the Flight Trip  \n",
    "**DAY** : Day of the Flight Trip  \n",
    "**DAY_OF_WEEK** : Day of week of the Flight Trip  \n",
    "**AIRLINE** : Airline Code   \n",
    "**FLIGHT_NUMBER** : Flight Number  \n",
    "**TAIL_NUMBER** : Tail Number  \n",
    "**ORIGIN_AIRPORT** : Origin airport code   \n",
    "**DESTINATION_AIRPORT** : Destination airport code   \n",
    "**SCHEDULED_DEPARTURE** : Scheduled departure time  \n",
    "**SCHEDULED_ARRIVAL** : Scheduled arrival time  \n",
    "**DEPARTURE_TIME** : Actual departure time  \n",
    "**ARRIVAL_TIME** : Actual arrival time  \n",
    "**DEPARTURE_DELAY** : Delay in departure times (in minutes)   \n",
    "**ARRIVAL_DELAY** : Delay in arrival times (in minutes)  \n",
    "**AIR_TIME, SCHEDULED_TIME, ELAPSED_TIME, DIVERTED** : Times of flight and whether it was diverted   \n",
    "**DISTANCE** : Flight distance    \n",
    "**AIR_SYSTEM_DELAY, SECURITY_DELAY, AIRLINE_DELAY, LATE_AIRCRAFT_DELAY, WEATHER_DELAY** : Possible types of delays    \n",
    "**TAXI_IN, TAXI_OUT, WHEELS_OFF, WHEELS_ON** : Misc attributes    \n",
    "**CANCELLED, CANCELLATION_REASON** : Whether flight was cancelled and reason    \n",
    "\n",
    "### Airlines  \n",
    "\n",
    "**IATA_CODE** - Airport IATA code  \n",
    "**AIRLINE** - Airline name  \n",
    "\n",
    "\n",
    "### Airports  \n",
    "\n",
    "**IATA_CODE** - Airport IATA code  \n",
    "**AIRPORT** - Airport  \n",
    "**CITY, STATE, COUNTRY** - City, state, and country of airport  \n",
    "**LATITUDE, LONGITUDE** - Latitude and longitude of airport \n",
    "\n",
    "### Regions\n",
    "N - North\n",
    "W - West\n",
    "M - Midwest\n",
    "S - South\n",
    "O - Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows and columns in each data set\n",
    "\n",
    "print (Airlines.shape)\n",
    "print (Airports.shape)\n",
    "print (Flights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Airports.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset Airports has a total of 322 records and 7 variables. It has all the pertinent information on geographical location of the airports (includes the city/state/country/longitude/latitude).  It is noticed that both Latitude and Longitude attributes are missing 3 values with a toal count of records for each at 319 instead of 322.  Below is an extraction of the first five records from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Airports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Airlines.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset Airlines has a total of 14 records and 2 variables. They are both categorical variables.  Below is an extraction of the first five records from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Airlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flights.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Flights dataset has 5.8M records and 31 variables.The dataset contains all information on the flights that took place in the US airports in 2015. It includes data on the date/time of the flights, flight identifiers like flight and tail numbers, the airports they took off from and landed at, if there were any delays, the time spent in delays and the reason for delays, and if the flights were cancelled or diverted because of the delays.  \n",
    "### Below is an extraction of the first five records for all the attributes in the flights dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>3. Data Quality</font>  \n",
    "Verify data quality: Explain any missing values, duplicate data, and outliers. Are those mistakes? How do you deal with these problems? Give justifications for your methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values Analysis - Flights Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values Analysis\n",
    "\n",
    "#Get the number of total records. Since flights is the biggest dataset, we'll check for missing data in it\n",
    "length = len(Flights)\n",
    "\n",
    "#Get percent NA and sort them\n",
    "missing_values = Flights.isnull().sum()/length\n",
    "missing_values = missing_values.sort_values(ascending=False)\n",
    "\n",
    "#Filter out Attributes with less than 0.1% NAs\n",
    "missing_values = missing_values[missing_values.values * 100 > .1].to_frame()\n",
    "\n",
    "#Create useful information in columns\n",
    "missing_values[\"Total_Missing\"] = (length * missing_values).astype(int)\n",
    "missing_values[\"Remaining\"] = length - missing_values[\"Total_Missing\"]\n",
    "\n",
    "#Little bit of aesthetic cleanup\n",
    "missing_values = missing_values.reset_index()\n",
    "missing_values = missing_values.rename(columns = {0: \"Percent_NA\", \"index\":\"Attribute\"}) \n",
    "missing_values.columns.name = \"Rank\"\n",
    "\n",
    "#Display\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn index name 'Rank' into a column named 'index' so we can order the data easier\n",
    "missing_values = missing_values.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "cutoff_percentage = .15\n",
    "\n",
    "with sns.plotting_context(font_scale=5.5):\n",
    "    plt.rcParams[\"figure.figsize\"] = [5,10]\n",
    "    fmtr = FuncFormatter(lambda x,pos: '{:.0%}'.format(x))\n",
    "    \n",
    "    piv = pd.pivot_table(missing_values,values=\"Percent_NA\",index=\"index\")\n",
    "    ax = sns.heatmap(piv, square=False, cmap=\"seismic\",\n",
    "                     vmin=0,vmax=1, center=cutoff_percentage,fmt='.0%',\n",
    "                     cbar_kws={'format': fmtr},\n",
    "                     yticklabels=missing_values[\"Attribute\"].tolist(), annot=True)\n",
    "    \n",
    "    ax.set_xlabel(\"\",fontsize=30)\n",
    "    ax.set_ylabel(\"Attribute Name\",fontsize=20)\n",
    "    ax.tick_params(labelsize=13)\n",
    "    ax.tick_params(axis=\"x\", labelsize = 20)\n",
    "    ax.set_title(label=\"Percentage of NA Values\\n\")\n",
    "    ax.title.set_size(30)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17 of the 31 attributes (or a little over half of the attributes) have missing values. We have to stress that the above missing values analysis on the flights dataset can be a bit misleading for our data. In the sense that, the attributes that show a high percentage of missing data do not necessarily correlate with bad data or data that we may want to impute for. For e.g. 82% of the records are missing a value for the variable weather delay. And that is expected, since not all flights get delayed due to weather.   \n",
    "### In other words, these are not missing data due to the dataset being unclean, but because not all flights flying out of all airports at all times will be cancelled or delayed due to weather, security reasons, air system delays.\n",
    "### For computational purposes of the lab we have decided to replace NaN with \"0\" values (this will be accomplished later on in this lab).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values Analysis -  Airlines Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/22257527/how-do-i-get-a-summary-count-of-missing-nan-data-by-column-in-pandas\n",
    "missing_values = Airlines.isnull().sum()/len(Airlines)*100\n",
    "missing_values = missing_values.sort_values(ascending=False) \n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Airlines dataset has no missing values in either of its attributes. This was expected since each airline will be designated an IATA Code and a name. These are necessary identifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values Analysis -  Airports Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/22257527/how-do-i-get-a-summary-count-of-missing-nan-data-by-column-in-pandas\n",
    "missing_values = Airports.isnull().sum()/len(Airports)*100\n",
    "missing_values = missing_values.sort_values(ascending=False) \n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Missing values \n",
    "missing_values = Airports.isnull().sum().reset_index()\n",
    "missing_values.columns = ['Variable Name', 'Number Missing Values']\n",
    "missing_values = missing_values[missing_values['Number Missing Values'] > 0] \n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Airports dataset is missing 3 values each for the Latitude and Longitutde attributes.  Below is a table that shows which airports had the missing values for longitude and latitude and other geographic information related to the airports.  For this lab's purposes we have decided not to delete them since in this lab we want to provide a high level overview/analysis of the three datasets used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to see which rows have the null values\n",
    "Airports.loc[Airports.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Analysis -  Flights Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import statistics\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def getNumOfOutliers(row, avg, std, alpha=.05):\n",
    "    x = 0\n",
    "    for cell in row:\n",
    "        if (cell>avg+std*stats.t.ppf(1-alpha, 999)) or (cell<avg-std*stats.t.ppf(1-alpha, 999)):\n",
    "            x=x+1\n",
    "    return x\n",
    "\n",
    "def getNumOfZeroes(row):\n",
    "    x = 0\n",
    "    for cell in row:\n",
    "        if cell==0:\n",
    "            x=x+1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_plot = Flights.loc[:,Flights.dtypes==float].columns\n",
    "\n",
    "outlier_mean = Flights[outlier_plot].apply(lambda row: statistics.mean(row))\n",
    "\n",
    "outlier_data = pd.DataFrame(data={'mean':outlier_mean},index=outlier_plot)\n",
    "\n",
    "outlier_data['std'] = Flights[outlier_plot].apply(lambda row: statistics.stdev(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data['NumberOfOutliers'] = Flights[outlier_plot].apply(lambda row: getNumOfOutliers(row, statistics.mean(row),statistics.stdev(row),.001 ))\n",
    "outlier_data['NumberOfExtremeOutliers'] = Flights[outlier_plot].apply(lambda row: getNumOfOutliers(row, statistics.mean(row),statistics.stdev(row),.0000000000000001 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data['NumberOfZeroes'] = Flights[outlier_plot].apply(lambda row: getNumOfZeroes(row))\n",
    "outlier_data['PctOfZeroes'] = Flights[outlier_plot].apply(lambda row: getNumOfZeroes(row)/Flights.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data = outlier_data.sort_values(by='NumberOfOutliers',ascending=False)\n",
    "display(HTML(outlier_data.head(10).to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6)) \n",
    "sns.heatmap(outlier_data[['NumberOfOutliers','NumberOfExtremeOutliers','NumberOfZeroes']].corr(),linewidths=0.5, annot=True); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code for analysing outliers did not render. It is extremely compute intensive, especially keeping in mind the size of our dataset. We were hoping to see if there was a correlation between enumber of outliers and number of zeroes.  In the interst of time and keeping in mind that this is a prelim analysis of the datasets, we have decided to include the code. We will run the outliers analysis in future labs.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>4. Simple Statistics</font>  \n",
    "Visualize appropriate statistics (e.g., range, mode, mean, median, variance, counts) for a subset of attributes. Describe anything meaningful you found from this or if you found something potentially interesting. Note: You can also use data from other sources for comparison. Explain why the statistics run are meaningful.\n",
    "\n",
    "### We will we be running simple statistics on the flights dataset only since the airports and airlines datasets have metadata and won't have any value for running simple statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flights.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change date of week from numbers to days\n",
    "Flights.YEAR.unique() #2015\n",
    "day_of_week_desc={\n",
    "    1:'Monday',\n",
    "    2:'Tuesday',\n",
    "    3:'Wednesday',\n",
    "    4:'Thursday',\n",
    "    5:'Friday',\n",
    "    6:'Saturday',\n",
    "    7:'Sunday'\n",
    "}\n",
    "Flights['DAYS_OF_WEEK']=Flights['DAY_OF_WEEK'].apply(lambda a:day_of_week_desc[a])\n",
    "Flights.head()['DAYS_OF_WEEK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change months from numbers to days\n",
    "#Flights.YEAR.unique() #2015\n",
    "#month_dict={\n",
    "#    1:'Jan',\n",
    "#    2:'Feb',\n",
    "#    3:'Mar',\n",
    "#   4:'Apr',\n",
    "#    5:'May',\n",
    "#    6:'Jun',\n",
    "#    7:'Jul',\n",
    "#    8:'Aug',\n",
    "#    9:'Sep',\n",
    "#    10:'Oct',\n",
    "#    11:'Nov',\n",
    "#    12:'Dec'\n",
    "#}\n",
    "#Flights['Month_desc']=Flights['MONTH'].apply(lambda a:month_dict[a])\n",
    "#Flights.head()['Month_desc']##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding year month day to get date e.g 2015-01-01\n",
    "Flights['DATE'] = pd.to_datetime(Flights[['YEAR','MONTH', 'DAY']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot( y=Flights[\"DAY_OF_WEEK\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(Flights[\"DAY_OF_WEEK\"], shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There appears to an approximately equal number of flights during the week on Monday through Friday. However, there appears to be a significant decrease number of flights occurring on Saturday with a small increase on Sunday. The possible indication for the higher number of flights during the week could be due to the business travelers flight to and from nationwide offices or due to ticket price dropping in the middle of the week. The Saturday and Sunday decrease may perhaps be an indication that less people travel during non-business days.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot( y=Flights[\"MONTH\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot( y=Flights[\"MONTH\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(Flights[\"MONTH\"], shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems flight trends tend to increase beginning in the Spring which could be coincide with Spring Break most University and K-12 receive time off from school during this time so there is an opportunity for families to travel.  The trend looks to dip slight in May, when most school in session then increase again in June. July is the height for flights which also follows along with he school session due to schools in most parts of the country being about for summer break. There is a noticeable decrease in August and continuing through the Fall when schools are back in session. The Flight rebound slightly in December and January but decrease drastically in February.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot( y=Flights[\"DISTANCE\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The largest amount of flights occurred below 700 miles then drops off drastically over 1300 miles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#air system delay\n",
    "Flights2 = Flights.loc[(Flights['AIR_SYSTEM_DELAY'] != False, ['ARRIVAL_DELAY'])]\n",
    "\n",
    "delays_airsys = Flights.loc[(Flights['AIR_SYSTEM_DELAY'] > 0, ['ARRIVAL_DELAY'])].sum()\n",
    "notdelayed_airsys = Flights.loc[(Flights['AIR_SYSTEM_DELAY'] <= 0, ['ARRIVAL_DELAY'])].sum()\n",
    "\n",
    "count_sys = Flights2[(Flights['AIR_SYSTEM_DELAY'] != False)].count()\n",
    "count_sys_pos =Flights.loc[(Flights['AIR_SYSTEM_DELAY'] > 0, ['ARRIVAL_DELAY'])].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# security delay\n",
    "Flights2 = Flights.loc[(Flights['SECURITY_DELAY'] != False, ['ARRIVAL_DELAY'])]\n",
    "\n",
    "delays_airsec = Flights.loc[(Flights['SECURITY_DELAY'] > 0, ['ARRIVAL_DELAY'])].sum()\n",
    "notdelayed_airsec = Flights.loc[(Flights['SECURITY_DELAY'] <= 0, ['ARRIVAL_DELAY'])].sum()\n",
    "\n",
    "count_sec = Flights2[(Flights['SECURITY_DELAY'] != False)].count()\n",
    "count_sec_pos = Flights.loc[(Flights['SECURITY_DELAY'] > 0, ['ARRIVAL_DELAY'])].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#airline delay\n",
    "Flights2 = Flights.loc[(Flights['AIRLINE_DELAY'] != False, ['ARRIVAL_DELAY'])]\n",
    "\n",
    "delays_airair = Flights.loc[(Flights['AIRLINE_DELAY'] > 0, ['ARRIVAL_DELAY'])].sum()\n",
    "notdelayed_airair = Flights.loc[(Flights['AIRLINE_DELAY'] <= 0, ['ARRIVAL_DELAY'])].sum()\n",
    "\n",
    "count_air = Flights2[(Flights['AIRLINE_DELAY'] != False)].count()\n",
    "count_air_pos = Flights.loc[(Flights['AIRLINE_DELAY'] > 0, ['ARRIVAL_DELAY'])].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# late arrival delay\n",
    "Flights2 = Flights.loc[(Flights['LATE_AIRCRAFT_DELAY'] != False, ['ARRIVAL_DELAY'])]\n",
    "\n",
    "delays_airlate = Flights.loc[(Flights['LATE_AIRCRAFT_DELAY'] > 0, ['ARRIVAL_DELAY'])].sum()\n",
    "notdelayed_airlate = Flights.loc[(Flights['LATE_AIRCRAFT_DELAY'] <= 0, ['ARRIVAL_DELAY'])].sum()\n",
    "\n",
    "count_late = Flights2[(Flights['LATE_AIRCRAFT_DELAY'] != False)].count()\n",
    "count_late_pos = Flights.loc[(Flights['LATE_AIRCRAFT_DELAY'] > 0, ['ARRIVAL_DELAY'])].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather delay\n",
    "Flights2 = Flights.loc[(Flights['WEATHER_DELAY'] != False, ['ARRIVAL_DELAY'])]\n",
    "\n",
    "delays_airweather = Flights.loc[(Flights['WEATHER_DELAY'] > 0, ['ARRIVAL_DELAY'])].sum()\n",
    "notdelayed_airweather = Flights.loc[(Flights['WEATHER_DELAY'] <= 0, ['ARRIVAL_DELAY'])].sum()\n",
    "\n",
    "count_weather = Flights2[(Flights['WEATHER_DELAY'] != False)].count()\n",
    "count_weather_pos = Flights.loc[(Flights['WEATHER_DELAY'] > 0, ['ARRIVAL_DELAY'])].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'Weather', 'Late Arrival', 'Carrier', 'Security', 'Air System'\n",
    "sizes = [delays_airweather, delays_airlate, delays_airair, delays_airsec, delays_airsys]\n",
    "explode = (0.1, 0.1, 0.1, 0.1, 0.1)\n",
    "colors=['magenta','green','yellow','blue','red']\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(8,8))\n",
    "ax1.pie(sizes, colors=colors,labels=labels, autopct='%1.2f%%',\n",
    "        shadow=True, explode=explode, startangle=90, center=(0,0))\n",
    "ax1.axis('equal')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>6. Explore Joint Attributes</font> \n",
    "Visualize relationships between attributes: Look at the attributes via scatter plots, correlation, cross-tabulation, group-wise averages, etc. as appropriate. Explain any interesting relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge airports and regions so that the states (and airports) are associated with a region\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "airports_regions = pd.merge(Airports, Regions, on='STATE', how='left')\n",
    "airports_regions.sort_values(by='IATA_CODE')\n",
    "airports_regions_final = airports_regions[['IATA_CODE','REGION']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge once for the Origin Region\n",
    "flights_regions_single = pd.merge(Flights, airports_regions_final, left_on='ORIGIN_AIRPORT', right_on='IATA_CODE')\n",
    "#Rename column\n",
    "flights_regions_single.rename(columns={'REGION':'ORIGIN_REGION'}, inplace=True)\n",
    "#Merge twice for the Destination Region\n",
    "flights_regions_double = pd.merge(flights_regions_single, airports_regions_final, left_on='DESTINATION_AIRPORT', right_on='IATA_CODE')\n",
    "#Drop extra columns from the merges\n",
    "flights_regions = flights_regions_double.drop(['IATA_CODE_x', 'IATA_CODE_y'], axis=1)\n",
    "#Rename column\n",
    "flights_regions.rename(columns={'REGION':'DESTINATION_REGION'}, inplace=True)\n",
    "#Sort dataframe based upon month, day, day of week, and departure time\n",
    "flights_regions_final = flights_regions.sort_values(['MONTH', 'DAY', 'DAY_OF_WEEK','SCHEDULED_DEPARTURE'], ascending=[True, True, True, True])\n",
    "#Run the line below to see the data at this point\n",
    "#flights_regions_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A variable of interest is that of regions. Each airport is associated with a state, and each state is associated with a region. We merged two documents containing these pieces of information to associate each airport with a region. Exploratory analysis is conducted on flights that are and are not associated with region changes (i.e. N-N or S-N).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create a dataframe of only delayed flights\n",
    "delayed_flights_only = flights_regions_final\n",
    "is_delayed = delayed_flights_only['ARRIVAL_DELAY'] > 0\n",
    "delayed_flights_only = delayed_flights_only[is_delayed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to label different target classes Delayed-3 or Cancelled-1 or Diverted-2, others = 0\n",
    "def f(row):\n",
    "    if row['CANCELLED'] == 1:\n",
    "        val = 'Cancelled'\n",
    "    elif row['DIVERTED'] == 1:\n",
    "        val = 'Diverted'\n",
    "    elif row['ARRIVAL_DELAY'] > 0:\n",
    "        val = 'Delayed'\n",
    "    else:\n",
    "        val = 'Unaffected'\n",
    "    return val\n",
    "\n",
    "#Run function on our dataset\n",
    "flights_regions_final['DELAYED_OR_CANCELLED'] = flights_regions_final.apply(f, axis=1)\n",
    "\n",
    "#Print out columns so that continuous attributes may be selected\n",
    "print(flights_regions_final.columns)\n",
    "scatter_columns = ['SCHEDULED_DEPARTURE', 'DEPARTURE_TIME', 'DEPARTURE_DELAY', 'TAXI_OUT',\n",
    "       'WHEELS_OFF', 'SCHEDULED_TIME', 'ELAPSED_TIME', 'AIR_TIME', 'DISTANCE',\n",
    "       'WHEELS_ON', 'TAXI_IN', 'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME',\n",
    "       'ARRIVAL_DELAY', 'DELAYED_OR_CANCELLED']\n",
    "scatter_columns_delay = ['SCHEDULED_DEPARTURE', 'DEPARTURE_TIME', 'DEPARTURE_DELAY', 'TAXI_OUT',\n",
    "       'WHEELS_OFF', 'SCHEDULED_TIME', 'ELAPSED_TIME', 'AIR_TIME', 'DISTANCE',\n",
    "       'WHEELS_ON', 'TAXI_IN', 'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME',\n",
    "       'ARRIVAL_DELAY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following commented code took too long to run, but it was to create a scatterplot matrix\n",
    "#sb.pairplot(flights_regions_final[scatter_columns], kind=\"scatter\")\n",
    "#scatter_matrix(flights_regions_final_sample[scatter_columns])\n",
    "#plt.show()\n",
    "\n",
    "#Function to create heatmap of correlations\n",
    "def heatMap(df, mirror):\n",
    "\n",
    "    # Create Correlation df\n",
    "    corr = df.corr()\n",
    "    # Plot figsize\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    # Generate Color Map\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "   \n",
    "    if mirror == True:\n",
    "       #Generate Heat Map, allow annotations and place floats in map\n",
    "       sns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n",
    "       #Apply xticks\n",
    "       plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "       #Apply yticks\n",
    "       plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "       #show plot\n",
    "\n",
    "    else:\n",
    "       # Drop self-correlations\n",
    "       dropSelf = np.zeros_like(corr)\n",
    "       dropSelf[np.triu_indices_from(dropSelf)] = True# Generate Color Map\n",
    "       colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "       # Generate Heat Map, allow annotations and place floats in map\n",
    "       sns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\", mask=dropSelf)\n",
    "       # Apply xticks\n",
    "       plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "       # Apply yticks\n",
    "       plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    # show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call heatMap for all flights with the specified continuous variables\n",
    "heatMap(flights_regions_final[scatter_columns], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The heat map correlation results are just as we would expect them to be. Variables that are obviously related to arrival delays are highly correlated with it. Other variables are correlated to each other that also make sense, such as wheels on and scheduled arrival.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call heatMap for only delayed flights with the specified continuous variables\n",
    "heatMap(delayed_flights_only[scatter_columns_delay], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The results here for delayed flights are essentially the same for all flights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>7. Explore Attribute Class </font> \n",
    "Identify and explain interesting relationships between features and the class you are trying to predict (i.e., relationships with variables and the target classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Region Change Arrival and Departure Delays\n",
    "\n",
    "#Create binary variable to indicate if a region change occurred during flight\n",
    "flights_regions_final['REGION_CHANGE'] = np.where(flights_regions_final['ORIGIN_REGION'] == flights_regions_final['DESTINATION_REGION'], 0, 1)\n",
    "\n",
    "#Add to delayed flights\n",
    "delayed_flights_only = flights_regions_final\n",
    "is_delayed = delayed_flights_only['ARRIVAL_DELAY'] > 0\n",
    "delayed_flights_only = delayed_flights_only[is_delayed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The below code shows different arrival and departure delays medians and means for several attributes: REGION_CHANGE, REGIONS_COMBINED, and PART_OF_DAY, all of which were created. They are displayed for all flights, and only delayed flights.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at mean and median Arrival and Departure Delays grouped by new Region_Change variable\n",
    "flights_regions_final.groupby('REGION_CHANGE', as_index=False)['ARRIVAL_DELAY'].median()\n",
    "flights_regions_final.groupby('REGION_CHANGE', as_index=False)['DEPARTURE_DELAY'].median()\n",
    "flights_regions_final.groupby('REGION_CHANGE', as_index=False)['ARRIVAL_DELAY'].mean()\n",
    "flights_regions_final.groupby('REGION_CHANGE', as_index=False)['DEPARTURE_DELAY'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at mean and median Arrival and Departure Delays grouped by new Region_Change variable for\n",
    "#delayed flights only\n",
    "delayed_flights_only.groupby('REGION_CHANGE', as_index=False)['ARRIVAL_DELAY'].median()\n",
    "delayed_flights_only.groupby('REGION_CHANGE', as_index=False)['DEPARTURE_DELAY'].median()\n",
    "delayed_flights_only.groupby('REGION_CHANGE', as_index=False)['ARRIVAL_DELAY'].mean()\n",
    "delayed_flights_only.groupby('REGION_CHANGE', as_index=False)['DEPARTURE_DELAY'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Arrival and Departure Regions to create one variable that captures both\n",
    "flights_regions_final['Regions_Combined'] = flights_regions_final.ORIGIN_REGION.map(str) + \"-\" + flights_regions_final.DESTINATION_REGION\n",
    "\n",
    "#Add to delayed flights\n",
    "delayed_flights_only = flights_regions_final\n",
    "is_delayed = delayed_flights_only['ARRIVAL_DELAY'] > 0\n",
    "delayed_flights_only = delayed_flights_only[is_delayed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean and Median by new combined region variable\n",
    "flights_regions_final.groupby('Regions_Combined', as_index=False)['DEPARTURE_DELAY'].mean()\n",
    "flights_regions_final.groupby('Regions_Combined', as_index=False)['ARRIVAL_DELAY'].mean()\n",
    "flights_regions_final.groupby('Regions_Combined', as_index=False)['DEPARTURE_DELAY'].median()\n",
    "flights_regions_final.groupby('Regions_Combined', as_index=False)['ARRIVAL_DELAY'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same but for only delayed flights \n",
    "\n",
    "#Mean and Median by new combined region variable for delayed flights only\n",
    "delayed_flights_only.groupby('Regions_Combined', as_index=False)['DEPARTURE_DELAY'].mean()\n",
    "delayed_flights_only.groupby('Regions_Combined', as_index=False)['ARRIVAL_DELAY'].mean()\n",
    "delayed_flights_only.groupby('Regions_Combined', as_index=False)['DEPARTURE_DELAY'].median()\n",
    "delayed_flights_only.groupby('Regions_Combined', as_index=False)['ARRIVAL_DELAY'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to label different times of the day Morning, Afternoon, Evening\n",
    "def t(row):\n",
    "    if row['SCHEDULED_DEPARTURE'] <= 1100:\n",
    "        val = 'Morning'\n",
    "    elif row['SCHEDULED_DEPARTURE'] > 1100 and row['SCHEDULED_DEPARTURE'] <= 1700:\n",
    "        val = 'Afternoon'\n",
    "    else:\n",
    "        val = 'Evening'\n",
    "    return val\n",
    "\n",
    "#Run function on our dataset\n",
    "flights_regions_final['PART_OF_DAY'] = flights_regions_final.apply(t, axis=1)\n",
    "#Sort the data again\n",
    "flights_regions_final = flights_regions_final.sort_values(['MONTH', 'DAY', 'DAY_OF_WEEK','SCHEDULED_DEPARTURE'], ascending=[True, True, True, True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add to delayed flights\n",
    "delayed_flights_only = flights_regions_final\n",
    "is_delayed = delayed_flights_only['ARRIVAL_DELAY'] > 0\n",
    "delayed_flights_only = delayed_flights_only[is_delayed]\n",
    "\n",
    "#Mean and Median by new part of day variable\n",
    "flights_regions_final.groupby('PART_OF_DAY', as_index=False)['DEPARTURE_DELAY'].mean()\n",
    "flights_regions_final.groupby('PART_OF_DAY', as_index=False)['ARRIVAL_DELAY'].mean()\n",
    "flights_regions_final.groupby('PART_OF_DAY', as_index=False)['DEPARTURE_DELAY'].median()\n",
    "flights_regions_final.groupby('PART_OF_DAY', as_index=False)['ARRIVAL_DELAY'].median()\n",
    "\n",
    "#Mean and Median by new part of day variable for delayed flights only\n",
    "delayed_flights_only.groupby('PART_OF_DAY', as_index=False)['DEPARTURE_DELAY'].mean()\n",
    "delayed_flights_only.groupby('PART_OF_DAY', as_index=False)['ARRIVAL_DELAY'].mean()\n",
    "delayed_flights_only.groupby('PART_OF_DAY', as_index=False)['DEPARTURE_DELAY'].median()\n",
    "delayed_flights_only.groupby('PART_OF_DAY', as_index=False)['ARRIVAL_DELAY'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code below creates various scatterplots and box plots to examine relationships among variables, as well as a relationship between variables and the target prediction. Distributions can be examined for skewness in the boxplots. Where applicable, a log scale is used for ARRIVAL_DELAY, so that the graph is more interpretable. The title of each graph displays what attributes are plotted, and if there is a scale change.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Scatterplot of Day of Week vs Arrival Delay\n",
    "sns.catplot(x=\"DAY_OF_WEEK\", y=\"ARRIVAL_DELAY\", data=delayed_flights_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot of Day of Week vs log(Arrival Delay) for delayed flights only\n",
    "g = sns.catplot(x=\"DAY_OF_WEEK\", y=\"ARRIVAL_DELAY\", kind=\"box\", data=delayed_flights_only)\n",
    "g.fig.get_axes()[0].set_yscale('log')\n",
    "g.fig.suptitle('Log(Arrival_Delay) vs Day of Week')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot of Airline vs log(Arrival Delay) of delayed flights only\n",
    "g = sns.catplot(x=\"AIRLINE\", y=\"ARRIVAL_DELAY\", kind=\"box\",\n",
    "            data=delayed_flights_only);\n",
    "g.fig.get_axes()[0].set_yscale('log')\n",
    "g.fig.suptitle('Log(Arrival_Delay) vs Airline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot of Regions Combined vs log(Arrival Delay) of delayed flights only\n",
    "g = sns.catplot(x=\"ARRIVAL_DELAY\", y=\"Regions_Combined\", kind=\"box\",\n",
    "            data=delayed_flights_only)\n",
    "g.fig.get_axes()[0].set_xscale('log')\n",
    "g.fig.suptitle('Log(Arrival_Delay) vs Regions_Combined')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The cell below displays the same graphs from above, but only for flights where the ARRIVAL_DELAY was less than 200 minutes. Accoring to the boxplots, 75% of the data is 200 or less, so examining this is useful.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat for only Arrival Delays below 200 minutes\n",
    "#Boxplots ARRIVAL_DELAY < 200\n",
    "\n",
    "#Grab only <200 delayed flights\n",
    "delayed_flights_only_200 = delayed_flights_only\n",
    "less_200 = delayed_flights_only_200['ARRIVAL_DELAY'] < 200\n",
    "delayed_flights_only_200 = delayed_flights_only_200[less_200]\n",
    "\n",
    "#Boxplot of Day of Week vs Arrival Delay for flights with <200 minute delays only\n",
    "g = sns.catplot(x=\"DAY_OF_WEEK\", y=\"ARRIVAL_DELAY\", kind=\"box\",\n",
    "            data=delayed_flights_only_200)\n",
    "g.fig.suptitle('200 < Arrival_Delay vs Day of Week')\n",
    "plt.show()\n",
    "\n",
    "#Boxplot of Airline vs Arrival Delay for flights with <200 minute delays only\n",
    "g = sns.catplot(x=\"AIRLINE\", y=\"ARRIVAL_DELAY\", kind=\"box\",\n",
    "            data=delayed_flights_only_200)\n",
    "g.fig.suptitle('200 < Arrival_Delay vs Airline')\n",
    "plt.show()\n",
    "\n",
    "#Boxplot of Regions Combined vs Arrival Delay for flights with <200 minute delays only\n",
    "g = sns.catplot(x=\"ARRIVAL_DELAY\", y=\"Regions_Combined\", kind=\"box\",\n",
    "            data=delayed_flights_only_200)\n",
    "g.fig.suptitle('200 < Arrival_Delay vs Regions Combined')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The cells below attempt to get a numerical summary for related variables. Flight arrival delays are binned and then counts are gathered. This helps in the visualization process**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define categories for Arrival Delay times    \n",
    "cats = ['0-100', '101-200', '201-300', '301-400', '401-500', '500+']\n",
    "bins = [0, 100, 200, 300, 400, 500, 2000]\n",
    "\n",
    "#Create bins and counts for each airline by the defined arrival delay bin\n",
    "binned_airline_delays = (delayed_flights_only.groupby(['AIRLINE', pd.cut(delayed_flights_only['ARRIVAL_DELAY'], bins, labels=cats)])\n",
    "         .size()\n",
    "         .unstack(fill_value=0))\n",
    "#Display bins and counts\n",
    "print(binned_airline_delays) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bins and counts for each region change by the defined arrival delay bin\n",
    "binned_regionchange_delays = (delayed_flights_only.groupby(['Regions_Combined', pd.cut(delayed_flights_only['ARRIVAL_DELAY'], bins, labels=cats)])\n",
    "         .size()\n",
    "         .unstack(fill_value=0))\n",
    "#Display bins and counts\n",
    "print(binned_regionchange_delays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following boxplots are usefull to compare several attributes at once. In this case, region_combined and airline are combined in one box plot against arrival delays.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Box plots by region and airline combined over arrival_delay           \n",
    "g = sns.catplot(x=\"ARRIVAL_DELAY\", y=\"Regions_Combined\", row=\"AIRLINE\",\n",
    "                kind=\"box\", orient=\"h\", height=5, aspect=4,\n",
    "                data=delayed_flights_only)\n",
    "g.set(xscale=\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>8. New Features</font> \n",
    "Are there other features that could be added to the data or created from existing features? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All new features that were created are examined in sections 6 and 7, for reference.\n",
    "New features that were created from the data include:\n",
    "    1. Adding a new variable to indicate whether a region change took place or not\n",
    "    during a flight (binary)\n",
    "    2. Adding a new variable to indicate what specific (if any) region change occurred during a flight (i.e. N-N, S-N, etc)\n",
    "    3. Adding a new variable to indicate whether a flight was delayed, diverted, cancelled, or none of the above (0-3)\n",
    "\n",
    "New features that could be added to the data could possibly be weather data, as well as getting some differences between existing variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>9. Exceptional Work</font> \n",
    "Provide additional analyses. One idea: implement dimensionality reduction, then visualize and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import PCA from sklearn and assign PCA variables\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "finalDf = pd.concat([principalDf, flights_regions_final[['DELAYED_OR_CANCELLED']]], axis = 1)\n",
    "finalDf_sample = finalDf.sample(frac=.01, random_state=1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define target values for comparisons, as well as graph colors\n",
    "targets = ['Cancelled', 'Diverted', 'Delayed', 'Unaffected']\n",
    "colors = ['r', 'g', 'b', 'y']\n",
    "#Define indices to keep for graphing upon comparison\n",
    "indicesToKeep = finalDf_sample['DELAYED_OR_CANCELLED'].isin(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PCA Plot\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = sns.scatterplot(x=\"principal component 1\", y=\"principal component 2\", data=finalDf_sample, hue='DELAYED_OR_CANCELLED', palette=colors )\n",
    "\n",
    "ax.set_title('PC 1 vs PC 2', fontsize = 20)\n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "#ax.legend(targets)\n",
    "ax.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
